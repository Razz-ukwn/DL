{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster-RCNN-Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "d5OcuI_Nynhv",
        "outputId": "8f98f65d-8ebf-48ac-8bae-1ccccc0e9750"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3e817d4318ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E9eTtvCaWMw"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "    \"\"\"Region proposal network\"\"\"\n",
        "\n",
        "    def __init__(self, channels, n_anchor):\n",
        "        super(RPN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "        self.reg_layer = nn.Conv2d(channels, n_anchor *4, 1, 1, 0)\n",
        "        self.cls_layer = nn.Conv2d(channels, n_anchor *2, 1, 1, 0) ##use softmax here. equally use sigmoid if replacing 2 with 1.\n",
        "\n",
        "        # conv sliding layer\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        # Regression layer\n",
        "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
        "        self.reg_layer.bias.data.zero_()\n",
        "        # classification layer\n",
        "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "        self.cls_layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        pred_anchor_locs = self.reg_layer(x)\n",
        "        pred_cls_scores = self.cls_layer(x)\n",
        "        #print(pred_cls_scores.shape, pred_anchor_locs.shape)\n",
        "        #Out:\n",
        "        #torch.Size([1, 18, 32, 50]) torch.Size([1, 36, 32, 50])\n",
        "        #print(out_map.shape)\n",
        "        return pred_anchor_locs, pred_cls_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVozblwPI6SA"
      },
      "source": [
        "class Fast_RCNN(nn.Module):\n",
        "    \"\"\"fast RCNN\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Fast_RCNN, self).__init__()\n",
        "        self.roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096),\n",
        "                                              nn.Linear(4096, 4096)])\n",
        "        self.cls_loc = nn.Linear(4096, (n_classes + 1) * 4) # (n_classes + 1 background. Each will have 4 co-ordinates)\n",
        "        self.cls_loc.weight.data.normal_(0, 0.01)\n",
        "        self.cls_loc.bias.data.zero_()\n",
        "        self.score = nn.Linear(4096, n_classes +1) # (n_classes + 1 background)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.roi_head_classifier(x)\n",
        "        roi_cls_loc = self.cls_loc(x)\n",
        "        roi_cls_score = self.score(x)\n",
        "        #print(roi_cls_loc.shape, roi_cls_score.shape)\n",
        "        #Out:\n",
        "        # torch.Size([128, 84]), torch.Size([128, 21])\n",
        "        return roi_cls_loc, roi_cls_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBf7H6MQ1Fyj"
      },
      "source": [
        "def create_fe_extractor():\n",
        "    \"\"\" return layers for backbone network and number of channels in the output_map\n",
        "    \"\"\"\n",
        "    dummy_img = torch.zeros((1, 3, height, width)).float()\n",
        "    model = torchvision.models.vgg16(pretrained=True)\n",
        "    fe = list(model.features)\n",
        "    req_features = []\n",
        "    k = dummy_img.clone()\n",
        "    for i in fe:\n",
        "         k = i(k)\n",
        "         #print(k.size())\n",
        "         if k.size()[3] < width//sub_sample:\n",
        "             break\n",
        "         req_features.append(i)\n",
        "         out_channels = k.size()[1]\n",
        "    #print(len(req_features)) #30\n",
        "    #print(out_channels) # 512\n",
        "    channels = out_channels\n",
        "\n",
        "    return req_features, channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsLW40Ft5dBP"
      },
      "source": [
        "def calc_anchors():\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)\n",
        "    #print(anchor_base)\n",
        "    ctr_y = sub_sample / 2.\n",
        "    ctr_x = sub_sample / 2.\n",
        "\n",
        "    #print(ctr_y, ctr_x)\n",
        "    # Out: (8, 8)\n",
        "    for i in range(len(ratios)):\n",
        "        for j in range(len(anchor_scales)):\n",
        "              h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "              w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
        "\n",
        "              index = i * len(anchor_scales) + j\n",
        "\n",
        "              anchor_base[index, 0] = ctr_y - h / 2.\n",
        "              anchor_base[index, 1] = ctr_x - w / 2.\n",
        "              anchor_base[index, 2] = ctr_y + h / 2.\n",
        "              anchor_base[index, 3] = ctr_x + w / 2.\n",
        "\n",
        "    fe_size_x = (width//sub_sample)\n",
        "    fe_size_y = (height//sub_sample)\n",
        "    #print(fe_size_x)\n",
        "    #print(fe_size_y)\n",
        "\n",
        "    ctr_x = np.arange(sub_sample, (fe_size_x+1) * sub_sample, sub_sample)\n",
        "    ctr_y = np.arange(sub_sample, (fe_size_y+1) * sub_sample, sub_sample)\n",
        "\n",
        "    ctr = np.zeros((len(ctr_x) * len(ctr_y),2))\n",
        "    index = 0\n",
        "    for x in range(len(ctr_x)):\n",
        "        for y in range(len(ctr_y)):\n",
        "            ctr[index, 1] = ctr_x[x] - 8\n",
        "            ctr[index, 0] = ctr_y[y] - 8\n",
        "            index +=1\n",
        "\n",
        "    anchors = np.zeros(((fe_size_x * fe_size_y * 9), 4))\n",
        "    index = 0\n",
        "    for c in ctr:\n",
        "       ctr_y, ctr_x = c\n",
        "       for i in range(len(ratios)):\n",
        "           for j in range(len(anchor_scales)):\n",
        "               h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "               w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
        "               anchors[index, 0] = ctr_y - h / 2.\n",
        "               anchors[index, 1] = ctr_x - w / 2.\n",
        "               anchors[index, 2] = ctr_y + h / 2.\n",
        "               anchors[index, 3] = ctr_x + w / 2.\n",
        "               index += 1\n",
        "\n",
        "    return anchors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7b6kvGj8NMW"
      },
      "source": [
        "def calc_valid_anchors_and_labels(anchors):\n",
        "    \"\"\" returns all anchors which aren't overlap the image border and label\n",
        "    \"\"\"\n",
        "\n",
        "    index_inside = np.where(#??????????????????????????????????????????????????????????????????????????????????????\n",
        "            (anchors[:, 0] >= 0) &\n",
        "            (anchors[:, 1] >= 0) &\n",
        "            (anchors[:, 2] <= 512) &\n",
        "            (anchors[:, 3] <= 800)\n",
        "        )[0]\n",
        "\n",
        "    #print(index_inside.shape)\n",
        "    #Out: (4400,)\n",
        "\n",
        "    label = np.empty((len(index_inside), ), dtype=np.int32)\n",
        "    label.fill(-1)\n",
        "    #print(label.shape)\n",
        "    #Out = (4400, )\n",
        "\n",
        "    valid_anchor_boxes = anchors[index_inside]\n",
        "    #print(valid_anchor_boxes.shape)\n",
        "    #Out = (4400, 4)\n",
        "\n",
        "    return label, valid_anchor_boxes, index_inside\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n52mlZcVMVhV"
      },
      "source": [
        "def calc_ious(bboxes1, bboxes2):\n",
        "    \"\"\" Calculate ious with vectorization\n",
        "    \"\"\"\n",
        "\n",
        "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n",
        "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n",
        "    xA = np.maximum(x11, np.transpose(x21))\n",
        "    yA = np.maximum(y11, np.transpose(y21))\n",
        "    xB = np.minimum(x12, np.transpose(x22))\n",
        "    yB = np.minimum(y12, np.transpose(y22))\n",
        "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
        "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
        "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
        "    ious = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
        "    return ious"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7--_qLWPVnj"
      },
      "source": [
        "def calc_anchor_locations_and_labels(valid_anchor_boxes, ious, label):\n",
        "    \"\"\" calculate the anchor locatios and anchor labels\n",
        "    \"\"\"\n",
        "\n",
        "    #case 1 the highest iou for each gt_box and its corresponding anchor box\n",
        "    gt_argmax_ious = ious.argmax(axis=0)\n",
        "    #print(gt_argmax_ious)\n",
        "    gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "    #print(gt_max_ious)\n",
        "\n",
        "    #case 2 the highest iou for each anchor box and its corresponding ground truth box\n",
        "    argmax_ious = ious.argmax(axis=1)\n",
        "    #print(argmax_ious.shape)\n",
        "    #print(argmax_ious)\n",
        "    max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
        "    #print(max_ious)\n",
        "\n",
        "    gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
        "    #print(gt_argmax_ious)\n",
        "\n",
        "    label[max_ious < neg_iou_threshold] = 0\n",
        "    label[gt_argmax_ious] = 1\n",
        "    label[max_ious >= pos_iou_threshold] = 1\n",
        "\n",
        "    n_pos = 0.5 * n_sample\n",
        "    #positive samples\n",
        "    pos_index = np.where(label == 1)[0]\n",
        "    if len(pos_index) > n_pos:\n",
        "        disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
        "        label[disable_index] = -1\n",
        "\n",
        "    #negative samples\n",
        "    n_neg = n_sample * np.sum(label == 1)\n",
        "    neg_index = np.where(label == 0)[0]\n",
        "    if len(neg_index) > n_neg:\n",
        "        disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
        "        label[disable_index] = -1\n",
        "\n",
        "    #For each anchor box, find the groundtruth object which has max_iou\n",
        "    max_iou_bbox = bbox[argmax_ious]\n",
        "\n",
        "    box_height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
        "    box_width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
        "    ctr_y = valid_anchor_boxes[:, 0] + 0.5 * box_height\n",
        "    ctr_x = valid_anchor_boxes[:, 1] + 0.5 * box_width\n",
        "    base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
        "    base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
        "    base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
        "    base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\n",
        "\n",
        "    #find the locs\n",
        "    eps = np.finfo(box_height.dtype).eps\n",
        "    box_height = np.maximum(box_height, eps)\n",
        "    box_width = np.maximum(box_width, eps)\n",
        "    dy = (base_ctr_y - ctr_y) / box_height\n",
        "    dx = (base_ctr_x - ctr_x) / box_width\n",
        "    dh = np.log(base_height / box_height)\n",
        "    dw = np.log(base_width / box_width)\n",
        "    anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "    #print(anchor_locs)\n",
        "\n",
        "    #final labels\n",
        "    anchor_labels = np.empty((len(anchors),), dtype=label.dtype)\n",
        "    anchor_labels.fill(-1)\n",
        "    anchor_labels[index_inside] = label\n",
        "\n",
        "    #final locations\n",
        "    anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
        "    anchor_locations.fill(0)\n",
        "    anchor_locations[index_inside, :] = anchor_locs\n",
        "\n",
        "    return anchor_labels, anchor_locations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTLBGg8Wr-ep"
      },
      "source": [
        "def generate_regions_of_interest(is_training):\n",
        "    \"\"\" generate_region_proposals\n",
        "    \"\"\"\n",
        "    n_pre_nms = n_train_pre_nms if is_training else n_test_pre_nms\n",
        "    n_post_nms = n_train_post_nms if is_training else n_test_post_nms\n",
        "\n",
        "    anc_height = anchors[:, 2] - anchors[:, 0]\n",
        "    anc_width = anchors[:, 3] - anchors[:, 1]\n",
        "    anc_ctr_y = anchors[:, 0] + 0.5 * anc_height\n",
        "    anc_ctr_x = anchors[:, 1] + 0.5 * anc_width\n",
        "\n",
        "    pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()\n",
        "    objectness_score_numpy = objectness_score[0].data.numpy()\n",
        "    dy = pred_anchor_locs_numpy[:, 0::4]\n",
        "    dx = pred_anchor_locs_numpy[:, 1::4]\n",
        "    dh = pred_anchor_locs_numpy[:, 2::4]\n",
        "    dw = pred_anchor_locs_numpy[:, 3::4]\n",
        "    ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
        "    ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
        "    h = np.exp(dh) * anc_height[:, np.newaxis]\n",
        "    w = np.exp(dw) * anc_width[:, np.newaxis]\n",
        "\n",
        "    roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=np.float32) #pred_anchor_locs.numpy.dtype) KOMISCH ?!?!?!!?!?!?!?\n",
        "    roi[:, 0::4] = ctr_y - 0.5 * h\n",
        "    roi[:, 1::4] = ctr_x - 0.5 * w\n",
        "    roi[:, 2::4] = ctr_y + 0.5 * h\n",
        "    roi[:, 3::4] = ctr_x + 0.5 * w\n",
        "\n",
        "    #img_size = (height, width) #Image size\n",
        "    #print(img_size)\n",
        "\n",
        "    roi[:, slice(0, 4, 2)] = np.clip(\n",
        "                roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
        "    roi[:, slice(1, 4, 2)] = np.clip(\n",
        "        roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
        "\n",
        "    #print(roi)\n",
        "\n",
        "    hs = roi[:, 2] - roi[:, 0]\n",
        "    ws = roi[:, 3] - roi[:, 1]\n",
        "    keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
        "    roi = roi[keep, :]\n",
        "    score = objectness_score_numpy[keep]\n",
        "    #print(score.shape)\n",
        "\n",
        "    ordered_scores = score.ravel().argsort()[::-1]\n",
        "    #print(ordered_scores)\n",
        "\n",
        "    ordered_scores = ordered_scores[:n_pre_nms]\n",
        "    roi = roi[ordered_scores, :]\n",
        "\n",
        "    #print(roi.shape)\n",
        "    #print(roi)\n",
        "\n",
        "    y1 = roi[:, 0]\n",
        "    x1 = roi[:, 1]\n",
        "    y2 = roi[:, 2]\n",
        "    x2 = roi[:, 3]\n",
        "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    order = ordered_scores.argsort()[::-1]\n",
        "    keep = []\n",
        "    while (order.size > 0):\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = w * h\n",
        "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(ovr <= nms_thresh)[0]\n",
        "        order = order[inds + 1]\n",
        "    keep = keep[:n_post_nms] # while training/testing , use accordingly\n",
        "    roi = roi[keep] # the final region proposals\n",
        "\n",
        "    return roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5jVFQWwEiAM"
      },
      "source": [
        "def create_region_proposals():\n",
        "    \"\"\" Create sample rois, ground truth roi labels and ground truth roi locations\n",
        "    \"\"\"\n",
        "\n",
        "    gt_assignment = ious.argmax(axis=1)\n",
        "    max_iou = ious.max(axis=1)\n",
        "    #print(gt_assignment)\n",
        "    #print(max_iou)\n",
        "\n",
        "    gt_roi_label = labels[gt_assignment]\n",
        "    #print(gt_roi_label) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "    pos_roi_per_image = int(n_sample * pos_ratio)\n",
        "    pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
        "    pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
        "    if pos_index.size > 0:\n",
        "        pos_index = np.random.choice(\n",
        "            pos_index, size=pos_roi_per_this_image, replace=False)\n",
        "    #print(pos_roi_per_this_image)\n",
        "    #print(pos_index)\n",
        "\n",
        "    neg_index = np.where((max_iou < neg_iou_thresh_hi) &\n",
        "                             (max_iou >= neg_iou_thresh_lo))[0]\n",
        "    neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
        "    neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
        "                                 neg_index.size))\n",
        "    if  neg_index.size > 0 :\n",
        "        neg_index = np.random.choice(\n",
        "            neg_index, size=neg_roi_per_this_image, replace=False)\n",
        "    #print(neg_roi_per_this_image)\n",
        "    #print(neg_index)\n",
        "    keep_index = np.append(pos_index, neg_index)\n",
        "    gt_roi_labels = gt_roi_label[keep_index]\n",
        "    gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
        "    sample_roi = roi[keep_index]\n",
        "    #print(sample_roi.shape)\n",
        "\n",
        "    bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
        "    #print(bbox_for_sampled_roi.shape)\n",
        "    #Out\n",
        "    #(128, 4)\n",
        "    h = sample_roi[:, 2] - sample_roi[:, 0]\n",
        "    w = sample_roi[:, 3] - sample_roi[:, 1]\n",
        "    ctr_y = sample_roi[:, 0] + 0.5 * h\n",
        "    ctr_x = sample_roi[:, 1] + 0.5 * w\n",
        "    base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
        "    base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
        "    base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height\n",
        "    base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width\n",
        "\n",
        "    eps = np.finfo(h.dtype).eps\n",
        "    h = np.maximum(h, eps)\n",
        "    w = np.maximum(w, eps)\n",
        "    dy = (base_ctr_y - ctr_y) / h\n",
        "    dx = (base_ctr_x - ctr_x) / w\n",
        "    dh = np.log(base_height / h)\n",
        "    dw = np.log(base_width / w)\n",
        "    gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "    #print(gt_roi_locs) ############################## now gt_roi_locs and gt_roi_labels\n",
        "\n",
        "    return sample_roi, gt_roi_labels, gt_roi_locs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTwDFzKrLYDO"
      },
      "source": [
        "def roi_pooling(sample_roi):\n",
        "    \"\"\" calculates roi pooling for sample_rois\n",
        "    \"\"\"\n",
        "\n",
        "    rois = torch.from_numpy(sample_roi).float()\n",
        "    roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
        "    roi_indices = torch.from_numpy(roi_indices).float()\n",
        "    #print(rois.shape, roi_indices.shape)\n",
        "    #Out:\n",
        "    #torch.Size([128, 4]) torch.Size([128])\n",
        "\n",
        "    indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
        "    xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
        "    indices_and_rois = xy_indices_and_rois.contiguous()\n",
        "    #print(xy_indices_and_rois.shape)\n",
        "    size = (7, 7)\n",
        "    adaptive_max_pool = nn.AdaptiveMaxPool2d(size)\n",
        "    output = []\n",
        "    rois = indices_and_rois.data.float()\n",
        "    rois[:, 1:].mul_(1/16.0) # Subsampling ratio\n",
        "    rois = rois.long()\n",
        "    num_rois = rois.size(0)\n",
        "    for i in range(num_rois):\n",
        "        roi = rois[i]\n",
        "        im_idx = roi[0]\n",
        "        im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
        "        output.append(adaptive_max_pool(im))\n",
        "    output = torch.cat(output, 0)\n",
        "    #print(output.size())\n",
        "    #Out:\n",
        "    # torch.Size([128, 512, 7, 7])\n",
        "    # Reshape the tensor so that we can pass it through the feed forward layer.\n",
        "    k = output.view(output.size(0), -1)\n",
        "    #print(k.shape)\n",
        "    #Out:\n",
        "    # torch.Size([128, 25088])\n",
        "    return k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWrNPzGBRb4X"
      },
      "source": [
        "def calc_rpn_loss():\n",
        "    \"\"\" calculate the rpns loss\n",
        "    \"\"\"\n",
        "\n",
        "    #rpn classification loss\n",
        "    rpn_loc = pred_anchor_locs[0]\n",
        "    rpn_score = pred_cls_scores[0]\n",
        "    gt_rpn_loc = torch.from_numpy(anchor_locations)\n",
        "    gt_rpn_score = torch.from_numpy(anchor_labels)\n",
        "    #print(rpn_loc.shape, rpn_score.shape, gt_rpn_loc.shape, gt_rpn_score.shape)\n",
        "    rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long(), ignore_index = -1)\n",
        "    #print(rpn_cls_loss)\n",
        "\n",
        "    #rpn regression loss\n",
        "    pos = gt_rpn_score > 0\n",
        "    mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
        "    #print(mask.shape)\n",
        "    mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
        "    mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
        "    #print(mask_loc_preds.shape, mask_loc_preds.shape)\n",
        "    x = torch.abs(mask_loc_targets - mask_loc_preds)\n",
        "    rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))\n",
        "    #print(rpn_loc_loss.sum())\n",
        "    N_reg = (gt_rpn_score >0).float().sum()\n",
        "    rpn_loc_loss = rpn_loc_loss.sum() / N_reg\n",
        "\n",
        "    rpn_loss = rpn_cls_loss + rpn_lambda*rpn_loc_loss\n",
        "\n",
        "    return rpn_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrjJA4_WRg_k"
      },
      "source": [
        "def calc_fast_rcnn_loss():\n",
        "    \"\"\" calculate the fast-RCNN classification and regression loss\n",
        "    \"\"\"\n",
        "\n",
        "    #Fast R-CNN classification loss\n",
        "    gt_roi_loc = torch.from_numpy(gt_roi_locs)\n",
        "    gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
        "    #print(gt_roi_loc.shape, gt_roi_label.shape)\n",
        "\n",
        "    roi_cls_loss = F.cross_entropy(roi_cls_score, gt_roi_label, ignore_index=-1)\n",
        "    #print(roi_cls_loss)\n",
        "\n",
        "    #Fast R-CNN Regression loss\n",
        "    n_sample = roi_cls_loc.shape[0]\n",
        "    roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
        "    #print(roi_loc.shape)\n",
        "    #Out:\n",
        "    #torch.Size([128, 21, 4])\n",
        "    roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
        "    #print(roi_loc.shape)\n",
        "    #Out:\n",
        "    #torch.Size([128, 4])\n",
        "    n_sample = roi_cls_loc.shape[0]\n",
        "    roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
        "    #print(roi_loc.shape)\n",
        "    roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
        "    #print(roi_loc.shape)\n",
        "    x_roi = torch.abs(gt_roi_loc - roi_loc)\n",
        "    roi_loc_loss = (((x_roi < 1).float() * 0.5 * x_roi ** 2) + ((x_roi >= 1).float() * (x_roi - 0.5)))\n",
        "    #print(roi_loc_loss.sum())\n",
        "\n",
        "    gt_rpn_score = torch.from_numpy(anchor_labels)\n",
        "    N_reg_roi = (gt_rpn_score > 0).float().sum()\n",
        "    roi_loc_loss = roi_loc_loss.sum() / N_reg_roi\n",
        "    roi_loss = roi_cls_loss + roi_lambda * roi_loc_loss\n",
        "    #print(roi_loss)\n",
        "\n",
        "    return roi_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5f6HB3t0QKr"
      },
      "source": [
        "input = Image.open(\"/content/drive/MyDrive/Computer Vision/0000f77c-6257be58.jpg\") #800x512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYLxVjs-0ib5"
      },
      "source": [
        "input.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3psriT70rno"
      },
      "source": [
        "height = 512 ##########################################################################################################################\n",
        "width = 800 ##########################################################################################################################\n",
        "img_size = (height, width)\n",
        "n_classes = 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_GpNPXK0sAB"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((height, width)),\n",
        "                                     transforms.ToTensor()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m8Rp2Cr0uRn"
      },
      "source": [
        "input_tensor = transform(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB8oXD_-0w_M"
      },
      "source": [
        "plt.imshow(input_tensor.permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyNT0kdA0yiO"
      },
      "source": [
        "image = torch.zeros((1, 3, height, width)).float() #800)).float()\n",
        "\n",
        "bbox = torch.FloatTensor([[20, 30, 300, 400], [100, 200, 400, 400]]) # [y1, x1, y2, x2] format\n",
        "labels = torch.LongTensor([6, 8]) # 0 represents background\n",
        "sub_sample = 16##########################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J1TTkZL1CHg"
      },
      "source": [
        "#Create Network Backbone\n",
        "req_features, channels = create_fe_extractor()\n",
        "faster_rcnn_fe_extractor = nn.Sequential(*req_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN5YSOD03R9D"
      },
      "source": [
        "print(faster_rcnn_fe_extractor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcdCRyLU3rGL"
      },
      "source": [
        "out_map = faster_rcnn_fe_extractor(image)\n",
        "print(out_map.size())\n",
        "#Out: torch.Size([1, 512, 32, 50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO7GbIXv4Fh6"
      },
      "source": [
        "#create anchors\n",
        "ratios = [0.5, 1, 2]###########################################################################################################\n",
        "anchor_scales = [8, 16, 32]#####################################################################################################\n",
        "n_anchor = len(ratios) * len(anchor_scales)\n",
        "print(n_anchor)\n",
        "\n",
        "anchors = calc_anchors()\n",
        "print(anchors.shape)\n",
        "    #Out: [14400, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FiJI-N65WT6"
      },
      "source": [
        "print(anchors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHc-7p2W7E--"
      },
      "source": [
        "label, valid_anchor_boxes, index_inside = calc_valid_anchors_and_labels(anchors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thf8dffR4ssL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYNtpmHn5JfS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sMH_TA8_MjK"
      },
      "source": [
        "ious = calc_ious(valid_anchor_boxes, bbox.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdO6XO1dxcuD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_qNWmlr5fTl"
      },
      "source": [
        "print(ious[:100])\n",
        "#Out: [22500, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i7-jjY7Mkdd"
      },
      "source": [
        "ious.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFb2T72AMmIj"
      },
      "source": [
        "pos_iou_threshold  = 0.7 #########################################################################\n",
        "neg_iou_threshold = 0.3 ##########################################################################\n",
        "\n",
        "n_sample = 256 ###########################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F47P8DdAUjY5"
      },
      "source": [
        "anchor_labels, anchor_locations = calc_anchor_locations_and_labels(valid_anchor_boxes, ious, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzixSF6gU3Y6"
      },
      "source": [
        "anchor_labels.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ppnfLCVWVJq"
      },
      "source": [
        "for loc in anchor_locations:\n",
        "  print(loc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsf5K4CrWbSm"
      },
      "source": [
        "rpn = RPN(channels, n_anchor) ############außerhalb der schleife packen !!!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Cvg0Cgk2Fo"
      },
      "source": [
        "pred_anchor_locs, pred_cls_scores = rpn(out_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfHJ9gSglIw8"
      },
      "source": [
        "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
        "print(pred_anchor_locs.shape)\n",
        "#Out: torch.Size([1, 22500, 4])\n",
        "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
        "print(pred_cls_scores.shape)\n",
        "#Out torch.Size([1, 50, 50, 18])\n",
        "objectness_score = pred_cls_scores.view(out_map.shape[0], out_map.shape[2], out_map.shape[3], 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
        "print(objectness_score.shape)\n",
        "#Out torch.Size([1, 22500])\n",
        "pred_cls_scores  = pred_cls_scores.view(out_map.shape[0], -1, 2)\n",
        "print(pred_cls_scores.shape)\n",
        "# Out torch.size([1, 22500, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhkUEX1FluEX"
      },
      "source": [
        "#is_training = True #!!!!!!!!?????????!!!!!!!!!???????????!!!!!!!!!!!??????????!!!!!!!!!?????????????????????????\n",
        "\n",
        "nms_thresh = 0.7 ###############################################################################################\n",
        "n_train_pre_nms = 12000 ########################################################################################\n",
        "n_train_post_nms = 2000 ########################################################################################\n",
        "n_test_pre_nms = 6000 ##########################################################################################\n",
        "n_test_post_nms = 300 ##########################################################################################\n",
        "min_size = 16 ##################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbsnhMm4l0w5"
      },
      "source": [
        "roi = generate_regions_of_interest(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbCloONSsW6Z"
      },
      "source": [
        "#Proposal targets\n",
        "n_sample = 128 #Number of samples to sample from roi, The default value is 128.\n",
        "pos_ratio = 0.25 #the number of positive examples out of the n_samples. The default values is 0.25.\n",
        "pos_iou_thresh = 0.5 #the number of positive examples out of the n_samples. The default values is 0.25.\n",
        "neg_iou_thresh_hi = 0.5 #The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\n",
        "neg_iou_thresh_lo = 0.0 #The overlap value bounding required to consider a region proposal as negitive [background object]."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVxG-sOauSy4"
      },
      "source": [
        "ious = calc_ious(roi,  bbox.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZuKPNbjvw2r"
      },
      "source": [
        "print(ious.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bFJZhFbvdDA"
      },
      "source": [
        "sample_roi, gt_roi_labels, gt_roi_locs = create_region_proposals()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQWoo-uyvfxz"
      },
      "source": [
        "sample_roi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2qqDQkhGBzU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XBsRGWMIh7j"
      },
      "source": [
        "fast_rcnn = Fast_RCNN()\n",
        "\n",
        "roi_pool_out = roi_pooling(sample_roi)\n",
        "roi_cls_loc, roi_cls_score = fast_rcnn(roi_pool_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeJ9x6INLf5l"
      },
      "source": [
        "#calc loss\n",
        "print(pred_anchor_locs.shape)\n",
        "print(pred_cls_scores.shape)\n",
        "print(anchor_locations.shape)\n",
        "print(anchor_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsztMIUmL1rM"
      },
      "source": [
        "rpn_lambda = 10#############################################################################\n",
        "rpn_loss = calc_rpn_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mZ0JfHEQzOv"
      },
      "source": [
        "print(roi_cls_loc.shape)\n",
        "print(roi_cls_score.shape)\n",
        "print(gt_roi_locs.shape)\n",
        "print(gt_roi_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uctr6KwXPUUt"
      },
      "source": [
        "roi_lambda = 10#############################################################\n",
        "roi_loss = calc_fast_rcnn_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH1MbGF0QwyX"
      },
      "source": [
        "#total_loss\n",
        "total_loss = rpn_loss + roi_loss\n",
        "print(total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lTXJShMOeh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY25Hiy2Njqn"
      },
      "source": [
        "############################################################################################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRR9rrQZOrmd"
      },
      "source": [
        "#set configs\n",
        "height = 512 ##########################################################################################################################\n",
        "width = 800 ##########################################################################################################################\n",
        "sub_sample = 16##########################################################################################\n",
        "ratios = [0.5, 1, 2]###########################################################################################################\n",
        "anchor_scales = [8, 16, 32]#####################################################################################################\n",
        "pos_iou_threshold  = 0.7 #########################################################################\n",
        "neg_iou_threshold = 0.3 ##########################################################################\n",
        "n_sample = 256 ###########################################################################################################\n",
        "#is_training = True #!!!!!!!!?????????!!!!!!!!!???????????!!!!!!!!!!!??????????!!!!!!!!!?????????????????????????\n",
        "\n",
        "nms_thresh = 0.7 ###############################################################################################\n",
        "n_train_pre_nms = 12000 ########################################################################################\n",
        "n_train_post_nms = 2000 ########################################################################################\n",
        "n_test_pre_nms = 6000 ##########################################################################################\n",
        "n_test_post_nms = 300 ##########################################################################################\n",
        "min_size = 16 ##################################################################################################\n",
        "\n",
        "#Proposal targets\n",
        "n_sample = 128 #Number of samples to sample from roi, The default value is 128.\n",
        "pos_ratio = 0.25 #the number of positive examples out of the n_samples. The default values is 0.25.\n",
        "pos_iou_thresh = 0.5 #the number of positive examples out of the n_samples. The default values is 0.25.\n",
        "neg_iou_thresh_hi = 0.5 #The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\n",
        "neg_iou_thresh_lo = 0.0 #The overlap value bounding required to consider a region proposal as negitive [background object].\n",
        "\n",
        "rpn_lambda = 10#############################################################################\n",
        "roi_lambda = 10#############################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvUzV10cOsO6"
      },
      "source": [
        "#prepare data\n",
        "input = Image.open(\"/content/drive/MyDrive/Computer Vision/0000f77c-6257be58.jpg\") #800x512\n",
        "img_size = (height, width)\n",
        "n_classes = 13\n",
        "transform = transforms.Compose([transforms.Resize((height, width)),\n",
        "                                     transforms.ToTensor()])\n",
        "input_tensor = transform(input)\n",
        "image = torch.zeros((1, 3, height, width)).float() #800)).float()\n",
        "\n",
        "bbox = torch.FloatTensor([[20, 30, 300, 400], [100, 200, 400, 400]]) # [y1, x1, y2, x2] format\n",
        "labels = torch.LongTensor([6, 8]) # 0 represents background"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhRcwL5r9KoI"
      },
      "source": [
        "#create anchors\n",
        "n_anchor = len(ratios) * len(anchor_scales)\n",
        "print(n_anchor)\n",
        "\n",
        "anchors = calc_anchors()\n",
        "print(anchors.shape)\n",
        "    #Out: [14400, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76GrLJc5SMB"
      },
      "source": [
        "#set network optimizer etc.\n",
        "# set a boolean flag that indicates whether a cuda capable GPU is available\n",
        "# we will need this for transferring our tensors to the device and\n",
        "# for persistent memory in the data loader\n",
        "is_gpu = torch.cuda.is_available()\n",
        "print(\"GPU is available:\", is_gpu)\n",
        "print(\"If you are receiving False, try setting your runtime to GPU\")\n",
        "\n",
        "# set the device to cuda if a GPU is available\n",
        "device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n",
        "\n",
        "#create Network Backbone\n",
        "req_features, channels = create_fe_extractor()\n",
        "faster_rcnn_fe_extractor = nn.Sequential(*req_features).to(device)\n",
        "print(faster_rcnn_fe_extractor)\n",
        "\n",
        "#create RPN\n",
        "rpn = RPN(channels, n_anchor).to(device)\n",
        "\n",
        "#create Fast-RCNN\n",
        "fast_rcnn = Fast_RCNN().to(device)\n",
        "params = list(faster_rcnn_fe_extractor.parameters()) + list(rpn.parameters()) + list(fast_rcnn.parameters())\n",
        "#model.load_state_dict(torch.load(\"/content/drive/My Drive/weights15.pt\"))#####################################################################\n",
        "\n",
        "#set optimizer for backpropagation\n",
        "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQSt1Pxf5jsQ"
      },
      "source": [
        "for i in range(1000):\n",
        "    print(\"step\",i)\n",
        "    #train loop (faster_rcnn_fe_extractor, anchors, )\n",
        "    label, valid_anchor_boxes, index_inside = calc_valid_anchors_and_labels(anchors)\n",
        "\n",
        "    ious = calc_ious(valid_anchor_boxes, bbox.numpy())\n",
        "    n_sample = 256 ###########################################################################################################\n",
        "    anchor_labels, anchor_locations = calc_anchor_locations_and_labels(valid_anchor_boxes, ious, label)\n",
        "\n",
        "    image.to(device)###########?!?!??!?!?!????????????????????!!!!!!!!!!!!!!?????????!!!!!!!!???????!!!!!!!!??????!\n",
        "    out_map = faster_rcnn_fe_extractor(image).to(device)\n",
        "\n",
        "    pred_anchor_locs, pred_cls_scores = rpn(out_map)\n",
        "\n",
        "    pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
        "    #print(pred_anchor_locs.shape)\n",
        "    #Out: torch.Size([1, 22500, 4])\n",
        "    pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
        "    #print(pred_cls_scores.shape)\n",
        "    #Out torch.Size([1, 50, 50, 18])\n",
        "    objectness_score = pred_cls_scores.view(out_map.shape[0], out_map.shape[2], out_map.shape[3], 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
        "    #print(objectness_score.shape)\n",
        "    #Out torch.Size([1, 22500])\n",
        "    pred_cls_scores  = pred_cls_scores.view(out_map.shape[0], -1, 2)\n",
        "    #print(pred_cls_scores.shape)\n",
        "    # Out torch.size([1, 22500, 2])\n",
        "\n",
        "    roi = generate_regions_of_interest(True) ############################!!!!!!!!!!!!!!!!!???????????????????!!!!!!!!!!!!!!!??????????????!!!!!!!!!!!!???????????!!!!!!\n",
        "\n",
        "    n_sample = 128 #Number of samples to sample from roi, The default value is 128.\n",
        "    ious = calc_ious(roi,  bbox.numpy())\n",
        "    sample_roi, gt_roi_labels, gt_roi_locs = create_region_proposals()\n",
        "\n",
        "    roi_pool_out = roi_pooling(sample_roi)\n",
        "    roi_cls_loc, roi_cls_score = fast_rcnn(roi_pool_out)\n",
        "\n",
        "    rpn_loss = calc_rpn_loss()\n",
        "    print(rpn_loss)\n",
        "\n",
        "    roi_loss = calc_fast_rcnn_loss()\n",
        "    print(roi_loss)\n",
        "    #total_loss\n",
        "    total_loss = rpn_loss + roi_loss\n",
        "    print(total_loss)\n",
        "\n",
        "    # compute gradient and do the SGD step\n",
        "    # we reset the optimizer with zero_grad to \"flush\" former gradients\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-bCfgvJAfQY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}